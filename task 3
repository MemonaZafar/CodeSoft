import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as transforms
from torchvision import models
from PIL import Image
import numpy as np

# custom dataset class for image and caption pairs

class CustomDataset(Dataset):
    def __init__(self, image_paths, captions, transform=None):
        self.image_paths = image_paths
        self.captions = captions
        self.transform = transform

    def __len__(self):
        return len(self.image_paths);
def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        image = Image.open(image_path).convert("RGB")
        if self.transform:
            image = self.transform(image);
        caption = self.captions[idx]
        return image, caption

#  now Preprocessing and data loading

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
# Replace this with your  data

image_paths = ["image1.jpg", "image2.jpg", ...]
captions = ["caption1", "caption2", ...]

# now Create data loader

dataset = CustomDataset(image_paths, captions, transform=transform);
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True);

# Pre-trained ResNet model for feature extraction

resnet = models.resnet50(pretrained=True);
modules = list(resnet.children())[:-2]
resnet = nn.Sequential(*modules);
for param in resnet.parameters():
    param.requires_grad = False

# RNN-based captioning model
class CaptioningModel(nn.Module):
    # Define your model architecture here
    pass

# Hyperparameters

vocab_size = len(vocab);
embed_size = 256
hidden_size = 512
num_layers = 1
learning_rate = 0.001
num_epochs = 10

# Initialize model and loss function

model = CaptioningModel(vocab_size, embed_size, hidden_size, num_layers).to(device);
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate);

# Training loop
for epoch in range(num_epochs):
    for images, captions in dataloader:
        images = images.to(device);
        captions = captions.to(device);

        optimizer.zero_grad()

# Extract image features

        features = resnet(images);
        features = features.view(features.size(0), -1)

        # Forward pass
        outputs = model(features, captions)

        # Compute loss
        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))

        # Backpropagation and optimization
        loss.backward()
        optimizer.step()

        # Print the loss every few iterations

# user Inference
def generate_caption(image_path, max_len=20):
    model.eval()
    image = Image.open(image_path).convert("RGB")
    image = transform(image).unsqueeze(0).to(device);
    features = resnet(image).view(1, -1);
    
    caption = ["<start>"]
    for _ in range(max_len):
        current_word = caption[-1]
        word_idx = vocab[current_word]
inputs = torch.tensor([word_idx]).to(device);
        with torch.no_grad():
            embeds = model.embedding(inputs);
            lstm_out, _ = model.lstm(torch.cat((features.unsqueeze(1), embeds.unsqueeze(1)), 1))
            outputs = model.fc(lstm_out);
            predicted = torch.argmax(outputs, 2);
            next_word = predicted[0][-1].item()
            caption.append(vocab.idx2word[next_word])
            if vocab.idx2word[next_word] == "<end>":
                break
    return " ".join(caption[1:-1])

# Example usage
image_path = "example.jpg"
caption = generate_caption(image_path);
print("Generated Caption:", caption)
